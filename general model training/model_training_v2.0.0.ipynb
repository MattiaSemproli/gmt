{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import os\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Input\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classes\n",
    "# class_names is a list of the name of the classes whose images are in the data folder\n",
    "# class_names is automatically generated from the names of the subfolders in the data folder\n",
    "# each subfolder in the data folder contains images of one class\n",
    "# for example if the data folder contains subfolders named 'cats' and 'dogs', then class_names = ['cats', 'dogs']\n",
    "# we save the number of classes in the variable num_classes for later use\n",
    "class_names = [class_name for class_name in os.listdir('data') if os.path.isdir(os.path.join('data', class_name))]\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Load the data from the data folder and class_names as labels for the classes\n",
    "# remap data as a dataset of (image, label) tuples, the image is divided by 255 to normalize the pixel values to the range [0, 1]\n",
    "data = tf.keras.utils.image_dataset_from_directory('data', class_names=class_names)\n",
    "data = data.map(lambda x, y: (x / 255, y))\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "# the train set contains 60% of the data\n",
    "# the validation set contains 30% of the data\n",
    "# the test set contains the remaining 10% of the data\n",
    "# the train, validation, and test sets are non-overlapping\n",
    "# for train we take the first 60% of the data\n",
    "# for val we skip the first 60% and take the next 30% of the data\n",
    "# for test we skip the first 60% + 30% and take the remaining 10% of the data\n",
    "train_size = int(len(data) * 0.6)\n",
    "val_size = int(len(data) * 0.3)\n",
    "test_size = len(data) - train_size - val_size\n",
    "train = data.take(train_size)\n",
    "val = data.skip(train_size).take(val_size)\n",
    "test = data.skip(train_size + val_size).take(test_size)\n",
    "\n",
    "# Define the input layer of the model, which is a 256x256x3 image\n",
    "inputs = Input(shape=(256, 256, 3))\n",
    "\n",
    "# Define the model, which is a sequential model, with the following layers:\n",
    "# 1. A 3x3 convolutional layer with 16 filters and a ReLU activation function\n",
    "# 2. A 2x2 max pooling layer\n",
    "# 3. A 3x3 convolutional layer with 32 filters and a ReLU activation function\n",
    "# 4. A 2x2 max pooling layer\n",
    "# 5. A 3x3 convolutional layer with 16 filters and a ReLU activation function\n",
    "# 6. A 2x2 max pooling layer\n",
    "# 7. A flatten layer\n",
    "# 8. A dense layer with 256 units and a ReLU activation function\n",
    "# 9. A dense layer with num_classes units and a softmax activation function\n",
    "model = Sequential([\n",
    "    inputs,\n",
    "    Conv2D(16, (3, 3), 1, activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(32, (3, 3), 1, activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(16, (3, 3), 1, activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model, using the Adam optimizer, the sparse categorical crossentropy loss function, and the accuracy metric\n",
    "# the Adam optimizer is a popular optimizer for training deep learning models\n",
    "# the sparse categorical crossentropy loss function is a popular loss function for multi-class classification problems\n",
    "# the accuracy metric is a popular metric for evaluating classification models\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Set up TensorBoard callback\n",
    "logdir = 'logs'\n",
    "tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "\n",
    "# Fit the model, using the train set for training and the validation set for validation\n",
    "# the model is trained for 20 epochs, and the TensorBoard callback is used to log training metrics\n",
    "hist = model.fit(train, epochs=20, validation_data=val, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history, including loss and accuracy\n",
    "plt.plot(hist.history['loss'], label='Training Loss')\n",
    "plt.plot(hist.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(hist.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in the format: model_name.keras\n",
    "model.save('model_name.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the model in another script, you need to load it using the load_model function from keras.models\n",
    "# Load the model\n",
    "loaded_model = load_model('model_name.keras')\n",
    "\n",
    "# Load the image we want to predict\n",
    "# convert the image to RGB because OpenCV loads images in BGR format\n",
    "# resize the image to 256x256, because the model expects images of this size\n",
    "# normalize the image to the range [0, 1], because the model expects normalized images\n",
    "# expand the dimensions of the image to (1, 256, 256, 3), because the model expects a batch of images\n",
    "# a bunch of images is represented as a 4D tensor with shape (batch_size, height, width, channels)\n",
    "# a single image is represented as a 4D tensor with shape (1, height, width, channels)\n",
    "# this is because the model is designed to take a batch of images as input, even if the batch size is 1\n",
    "# predict the class of the image using the model\n",
    "# then do what you want with the prediction, the prediction will be a list of probabilities for each class\n",
    "image = cv.imread('test1.jpg')\n",
    "image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "resized = cv.resize(image, (256, 256))\n",
    "normalized = resized / 255.0\n",
    "expanded = np.expand_dims(normalized, axis=0)\n",
    "predict = model.predict(expanded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
